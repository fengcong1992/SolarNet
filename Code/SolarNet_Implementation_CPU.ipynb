{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, sys, pickle\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import multi_gpu_model\n",
    "import tensorflow as tf\n",
    "import subprocess, argparse\n",
    "\n",
    "sys.path.append('/lustre/eaglefs/projects/pvdetect/SkyImage/SolarNet4/Git/Code')\n",
    "from get_model import *\n",
    "from get_generators import *\n",
    "\n",
    "\n",
    "# def get_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--img_size\", type=int, default = 256, help=\"Input sky image resolution\")\n",
    "#     parser.add_argument(\"--no_img\", type=int, default= 2, help=\"Length of input image sequence\")\n",
    "#     parser.add_argument(\"--train_batchsize\", type=int, default= 64)\n",
    "#     parser.add_argument(\"--validation_batchsize\", type=int, default= 64)\n",
    "#     parser.add_argument(\"--test_batchsize\", type=int, default= 1)\n",
    "#     args = parser.parse_args()\n",
    "#     return args\n",
    "# args = get_args()\n",
    "\n",
    "# get parameters that should be received from the terminal command line for HPC job submissions\n",
    "img_size = 128\n",
    "no_img = 2\n",
    "train_batchsize = 8\n",
    "validation_batchsize = 8\n",
    "test_batchsize = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 2)\n",
      "(54, 2)\n",
      "(54, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                 Image     Target\n",
       "0   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.743735\n",
       "1   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.491311\n",
       "2   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.263644\n",
       "3   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.041370\n",
       "4   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.831888\n",
       "5   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.647980\n",
       "6   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.872622\n",
       "7   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.861904\n",
       "8   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.791358\n",
       "9   [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.970264\n",
       "10  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.902137\n",
       "11  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.890267\n",
       "12  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.028943\n",
       "13  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.024183\n",
       "14  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.906880\n",
       "15  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.033203\n",
       "16  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.999411\n",
       "17  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.037502\n",
       "18  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.077585\n",
       "19  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.074708\n",
       "20  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.055616\n",
       "21  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.081909\n",
       "22  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.075075\n",
       "23  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.038533\n",
       "24  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.059528\n",
       "25  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.051401\n",
       "26  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.042417\n",
       "27  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.047154\n",
       "28  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.047752\n",
       "29  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.048007\n",
       "30  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.035682\n",
       "31  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.031442\n",
       "32  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.019247\n",
       "33  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.033821\n",
       "34  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.077325\n",
       "35  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.113003\n",
       "36  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.165197\n",
       "37  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.148782\n",
       "38  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.113260\n",
       "39  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.140481\n",
       "40  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.235952\n",
       "41  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.313381\n",
       "42  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.292510\n",
       "43  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.051161\n",
       "44  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.988318\n",
       "45  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.102779\n",
       "46  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.280987\n",
       "47  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.382925\n",
       "48  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.179072\n",
       "49  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   0.962573\n",
       "50  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.440920\n",
       "51  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   1.153001\n",
       "52  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...   2.160666\n",
       "53  [/lustre/eaglefs/projects/pvdetect/SkyImage/Da...  10.000000>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(os.path.join('/lustre/eaglefs/projects/pvdetect/SkyImage/SolarNet4/Git/Data', 'Data_3Days_eagle.pkl'),'rb')\n",
    "df_train = pickle.load(file)\n",
    "df_validation = pickle.load(file)\n",
    "df_test = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "# check the data frame shape\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# look into the data frame\n",
    "df_train.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {'batch_size': train_batchsize,\n",
    "           'dim': (img_size, img_size, 3 * no_img),\n",
    "           'channel_IMG': 3,\n",
    "           'shuffle': True,\n",
    "           'iftest': False}\n",
    "params2 = {'batch_size': validation_batchsize,\n",
    "           'dim': (img_size, img_size, 3 * no_img),\n",
    "           'channel_IMG': 3,\n",
    "           'iftest': False}\n",
    "params3 = {'batch_size': test_batchsize,\n",
    "           'dim': (img_size, img_size, 3 * no_img),\n",
    "           'channel_IMG': 3,\n",
    "           'shuffle': False,\n",
    "           'iftest': False}\n",
    "# build data generators\n",
    "train_generator = DataGenerator_SCNN(df_train, **params1)\n",
    "validation_generator = DataGenerator_SCNN(df_validation, **params2)\n",
    "test_generator = DataGenerator_SCNN(df_test, **params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cfeng/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/cfeng/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      3520      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,716,416\n",
      "Trainable params: 14,716,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    (None, 4, 4, 512)         14716416  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,814,081\n",
      "Trainable params: 16,814,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the SolarNet\n",
    "conv_base = SCNN(input_shape=(img_size, img_size, 3 * no_img))\n",
    "conv_base.summary() # check base model, a SCNN is used here for demonstration\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(256, activation='linear'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "optimizer = optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "class MyModelCheckPoint(ModelCheckpoint):\n",
    "    def __init__(self, singlemodel, *args, **kwargs):\n",
    "        self.singlemodel = singlemodel\n",
    "        super(MyModelCheckPoint, self).__init__(*args, **kwargs)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model = self.singlemodel\n",
    "        super(MyModelCheckPoint, self).on_epoch_end(epoch, logs)\n",
    "checkpointer1 = MyModelCheckPoint(model, filepath=\"BestSingleModel.hdf5\", save_best_only=True, verbose=1)\n",
    "# HPC parallel computing\n",
    "# parallel_model = multi_gpu_model(model, gpus=n_GPUs)\n",
    "# parallel_model.compile(loss='mean_absolute_error',  # mean_squared_error\n",
    "#                        optimizer=optimizer,\n",
    "#                        metrics=['mae'])\n",
    "model.compile(loss='mean_absolute_error',  # mean_squared_error\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-10 15:49:49.430750: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-10 15:49:49.430797: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-10 15:49:49.430839: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (europa-int): /proc/driver/nvidia/version does not exist\n",
      "2021-11-10 15:49:49.431448: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2021-11-10 15:49:49.449293: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "2021-11-10 15:49:49.458561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ef691a1280 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-10 15:49:49.458607: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cfeng/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 6s 999ms/step - loss: 1.2741 - mae: 1.2741 - val_loss: 1.1037 - val_mae: 1.3242\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.10365, saving model to BestSingleModel.hdf5\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 5s 893ms/step - loss: 1.0204 - mae: 1.0204 - val_loss: 0.9491 - val_mae: 1.2183\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.10365 to 0.94906, saving model to BestSingleModel.hdf5\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 5s 893ms/step - loss: 1.0423 - mae: 1.0423 - val_loss: 0.8143 - val_mae: 0.8932\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94906 to 0.81433, saving model to BestSingleModel.hdf5\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 5s 899ms/step - loss: 0.5750 - mae: 0.5750 - val_loss: 0.1677 - val_mae: 0.3913\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.81433 to 0.16773, saving model to BestSingleModel.hdf5\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 5s 900ms/step - loss: 0.2992 - mae: 0.2992 - val_loss: 0.1831 - val_mae: 0.3055\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16773\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 5s 890ms/step - loss: 0.3715 - mae: 0.3715 - val_loss: 1.1757 - val_mae: 0.4231\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16773\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 5s 873ms/step - loss: 0.3400 - mae: 0.3400 - val_loss: 0.3255 - val_mae: 0.3392\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16773\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 5s 888ms/step - loss: 0.3489 - mae: 0.3489 - val_loss: 0.1865 - val_mae: 0.4039\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16773\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 4s 746ms/step - loss: 0.3366 - mae: 0.3366 - val_loss: 0.0787 - val_mae: 0.2950\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16773 to 0.07866, saving model to BestSingleModel.hdf5\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 5s 830ms/step - loss: 0.3263 - mae: 0.3263 - val_loss: 0.1049 - val_mae: 0.3403\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07866\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 5s 893ms/step - loss: 0.3159 - mae: 0.3159 - val_loss: 0.2794 - val_mae: 0.2892\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07866\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 5s 891ms/step - loss: 0.3169 - mae: 0.3169 - val_loss: 0.1440 - val_mae: 0.3210\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07866\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 5s 871ms/step - loss: 0.3214 - mae: 0.3214 - val_loss: 1.0563 - val_mae: 0.3193\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07866\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 5s 789ms/step - loss: 0.1358 - mae: 0.1358 - val_loss: 0.3071 - val_mae: 0.1722\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07866\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 5s 857ms/step - loss: 0.3209 - mae: 0.3209 - val_loss: 0.0694 - val_mae: 0.3118\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07866 to 0.06944, saving model to BestSingleModel.hdf5\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 5s 874ms/step - loss: 0.3067 - mae: 0.3067 - val_loss: 0.3191 - val_mae: 0.3461\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06944\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 5s 886ms/step - loss: 0.3036 - mae: 0.3036 - val_loss: 0.1807 - val_mae: 0.3210\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06944\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 5s 894ms/step - loss: 0.3001 - mae: 0.3001 - val_loss: 0.1393 - val_mae: 0.2909\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06944\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 5s 878ms/step - loss: 0.3211 - mae: 0.3211 - val_loss: 0.1043 - val_mae: 0.3390\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06944\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 5s 875ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.0988 - val_mae: 0.3150\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06944\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 5s 766ms/step - loss: 0.3212 - mae: 0.3212 - val_loss: 0.0606 - val_mae: 0.3255\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06944 to 0.06060, saving model to BestSingleModel.hdf5\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 5s 788ms/step - loss: 0.3146 - mae: 0.3146 - val_loss: 0.3197 - val_mae: 0.3360\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06060\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 5s 882ms/step - loss: 0.3109 - mae: 0.3109 - val_loss: 0.1907 - val_mae: 0.2914\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06060\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 5s 883ms/step - loss: 0.3119 - mae: 0.3119 - val_loss: 0.3125 - val_mae: 0.3335\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.06060\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 5s 872ms/step - loss: 0.2770 - mae: 0.2770 - val_loss: 0.2324 - val_mae: 0.3304\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06060\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 5s 886ms/step - loss: 0.2955 - mae: 0.2955 - val_loss: 0.0486 - val_mae: 0.3159\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.06060 to 0.04858, saving model to BestSingleModel.hdf5\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 5s 775ms/step - loss: 0.1362 - mae: 0.1362 - val_loss: 0.2482 - val_mae: 0.3379\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.04858\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 5s 866ms/step - loss: 0.1305 - mae: 0.1305 - val_loss: 0.1448 - val_mae: 0.1681\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.04858\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 5s 902ms/step - loss: 0.1289 - mae: 0.1289 - val_loss: 1.1008 - val_mae: 0.3150\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.04858\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 5s 890ms/step - loss: 0.1259 - mae: 0.1259 - val_loss: 0.1413 - val_mae: 0.3002\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.04858\n",
      "Succss in loading the best single model\n"
     ]
    }
   ],
   "source": [
    "# Train the SolarNet\n",
    "os.chdir('/lustre/eaglefs/projects/pvdetect/SkyImage/SolarNet4/Git/Results')\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=int(df_train.shape[0] / train_batchsize),\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=int(df_validation.shape[0] / validation_batchsize),\n",
    "                              callbacks=[checkpointer1],\n",
    "                              verbose=1,\n",
    "                              workers=6,\n",
    "                              use_multiprocessing=False)\n",
    "model.save('Last_SingleModel.hdf5')\n",
    "\n",
    "# predict\n",
    "try:\n",
    "    model.load_weights('BestSingleModel.hdf5')\n",
    "    print('Succss in loading the best single model')\n",
    "except:\n",
    "    print('Fail to load the best single model')\n",
    "    pass\n",
    "\n",
    "y_hat = model.predict_generator(generator=test_generator, steps=df_test.shape[0] / test_batchsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SkyImage1",
   "language": "python",
   "name": "skyimage1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
